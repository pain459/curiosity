{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from moviepy import VideoFileClip\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import whisper\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Ensure CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract audio and frames\n",
    "def extract_audio_and_frames(video_path, frame_rate=1):\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio_path = \"audio.wav\"\n",
    "    clip.audio.write_audiofile(audio_path)\n",
    "    frames = []\n",
    "    for t in range(0, int(clip.duration), frame_rate):\n",
    "        frame = clip.get_frame(t)\n",
    "        frames.append(frame)\n",
    "    return audio_path, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe audio using Whisper\n",
    "def transcribe_audio(audio_path):\n",
    "    model = whisper.load_model(\"base\").to(device)\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect explicit language\n",
    "def detect_explicit_language(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\", device=0)\n",
    "    \n",
    "    # Break text into chunks of max length 512 tokens\n",
    "    max_length = 512\n",
    "    tokens = tokenizer(text, truncation=False, return_tensors=\"pt\")\n",
    "    token_chunks = torch.split(tokens['input_ids'], max_length, dim=1)\n",
    "    \n",
    "    explicit_results = []\n",
    "    for chunk in token_chunks:\n",
    "        truncated_text = tokenizer.decode(chunk.squeeze().tolist())\n",
    "        results = classifier(truncated_text)\n",
    "        explicit_results.extend([res for res in results if res['label'] == 'EXPLICIT'])\n",
    "    return explicit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and detect explicit visuals in frames\n",
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    return cv2.resize(frame, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_explicit_frames(frames):\n",
    "    model = YOLO(\"yolov8n.pt\")  # Replace with fine-tuned model\n",
    "    explicit_frames = []\n",
    "    for idx, frame in enumerate(frames):\n",
    "        frame = preprocess_frame(frame)\n",
    "        results = model.predict(frame, device=device)\n",
    "        if any(res.name == \"explicit\" for res in results[0].boxes):\n",
    "            explicit_frames.append(idx)\n",
    "    return explicit_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze video and combine results\n",
    "def analyze_video(video_path):\n",
    "    audio_path, frames = extract_audio_and_frames(video_path)\n",
    "    \n",
    "    # Audio analysis\n",
    "    transcription = transcribe_audio(audio_path)\n",
    "    explicit_text = detect_explicit_language(transcription)\n",
    "    \n",
    "    # Visual analysis\n",
    "    explicit_frames = detect_explicit_frames(frames)\n",
    "    \n",
    "    return {\"explicit_text\": explicit_text, \"explicit_frames\": explicit_frames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "def save_results(results, output_path=\"explicit_timings.txt\"):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"E:\\\\Movies\\\\GWLG\\\\GWLG.mkv\"  # Replace with your video file\n",
    "    results = analyze_video(video_path)\n",
    "    save_results(results)\n",
    "    print(\"Analysis complete. Results saved to explicit_timings.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
