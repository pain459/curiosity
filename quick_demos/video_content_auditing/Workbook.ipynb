{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries.\n",
    "# pip install moviepy opencv-python ffmpeg transformers pytorch torchvision speechrecognition librosa huggingface_hub\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract video components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip\n",
    "\n",
    "def extract_audio_and_frames(video_path, frame_rate=1):\n",
    "    clip = VideoFileClip(video_path)\n",
    "    # Extract audio\n",
    "    audio_path = \"audio.wav\"\n",
    "    clip.audio.write_audiofile(audio_path)\n",
    "    # Extract frames\n",
    "    frames = []\n",
    "    for t in range(0, int(clip.duration), frame_rate):\n",
    "        frame = clip.get_frame(t)\n",
    "        frames.append(frame)\n",
    "    return audio_path, frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\src_git\\curiosity\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from transformers import pipeline\n",
    "\n",
    "# Transcribe audio\n",
    "def transcribe_audio(audio_path):\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result['text']\n",
    "\n",
    "# Explicit language detection\n",
    "def detect_explicit_language(text):\n",
    "    classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "    results = classifier(text)\n",
    "    return [res for res in results if res['label'] == 'EXPLICIT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\kumar\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Detect explicit visuals in frames\n",
    "def detect_explicit_frames(frames):\n",
    "    model = YOLO('yolov8n.pt')  # Replace with a fine-tuned model\n",
    "    explicit_frames = []\n",
    "    for idx, frame in enumerate(frames):\n",
    "        results = model.predict(frame)\n",
    "        if \"explicit\" in [res['name'] for res in results.xyxy[0]]:\n",
    "            explicit_frames.append(idx)\n",
    "    return explicit_frames\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path):\n",
    "    audio_path, frames = extract_audio_and_frames(video_path)\n",
    "    transcription = transcribe_audio(audio_path)\n",
    "    explicit_text = detect_explicit_language(transcription)\n",
    "    explicit_frames = detect_explicit_frames(frames)\n",
    "    return {\"explicit_text\": explicit_text, \"explicit_frames\": explicit_frames}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, output_path=\"explicit_timings.txt\"):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for key, value in results.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    video_path = \"test_video.mp4\"  # Replace with your video file\n",
    "    results = analyze_video(video_path)\n",
    "    save_results(results)\n",
    "    print(\"Analysis complete. Results saved to explicit_timings.txt.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
